Лабораторные работы по курсу "Фреймворки искуственного интеллекта".  
Студент группы M8О-401Б-22 Шляхтуров Александр Викторович.

ЛР1 - KNN
ЛР2 - Линейная и логистическая регрессия
ЛР3 - Решающее дерево
ЛР4 - Случаный лес
ЛР5 - Градиентный бустинг

Так же в репозитории предоставлены .csv файлы с датасетами, на которых были осуществленны эксперименты.



Результаты классификации:

| Модель                         | Подход                     | Метрика  | Значение |
|--------------------------------|----------------------------|----------|----------|
| KNN                            | Бейзлайн                   | Accuracy | 0.91     |
| KNN                            | Улучшенный бейзлайн        | Accuracy | 0.95     |
| KNN                            | Имплементация              | Accuracy | 0.9833   |
| KNN                            | Улучшенная имплементация   | Accuracy | 1.00     |
| Логистическая регрессия        | Бейзлайн                   | Accuracy | 0.9667   |
| Логистическая регрессия        | Улучшенный бейзлайн        | Accuracy | 0.9667   |
| Логистическая регрессия        | Имплементация              | Accuracy | 1.00     |
| Логистическая регрессия        | Улучшенная имплементация   | Accuracy | 1.00     |
| Дерево классификации           | Бейзлайн                   | Accuracy | 0.9667   |
| Дерево классификации           | Улучшенный бейзлайн        | Accuracy | 0.9667   |
| Дерево классификации           | Имплементация              | Accuracy | 0.9660   |
| Дерево классификации           | Улучшенная имплементация   | Accuracy | 0.9750   |
| Случайный лес                  | Бейзлайн                   | Accuracy | 0.9583   |
| Случайный лес                  | Улучшенный бейзлайн        | Accuracy | 0.9583   |
| Случайный лес                  | Имплементация              | Accuracy | 0.9583   |
| Случайный лес                  | Улучшенная имплементация   | Accuracy | 0.9722   |
| Бустинг                        | Бейзлайн                   | Accuracy | 0.9500   |
| Бустинг                        | Улучшенный бейзлайн        | Accuracy | 0.9583   |
| Бустинг                        | Имплементация              | Accuracy | 0.9500   |
| Бустинг                        | Улучшенная имплементация   | Accuracy | 0.9598   |

Результаты регрессии:  
  
  
| Модель                    | Подход                     | R²     | MAE    |
|---------------------------|----------------------------|--------|--------|
| KNN                       | Бейзлайн                   | 0.7394 | 0.0546 |
| KNN                       | Улучшенный бейзлайн        | 0.7547 | 0.0535 |
| KNN                       | Имплементация              | 0.7394 | 0.0546 |
| KNN                       | Улучшенная имплементация   | 0.7447 | 0.0546 |
| Линейная регрессия        | Бейзлайн                   | 0.7956 | 0.0495 |
| Линейная регрессия        | Улучшенный бейзлайн        | 0.7915 | 0.0498 |
| Линейная регрессия        | Имплементация              | 0.7956 | 0.0495 |
| Линейная регрессия        | Улучшенная имплементация   | 0.8114 | 0.0489 |
| Дерево регрессии          | Бейзлайн                   | 0.5091 | 0.0755 |
| Дерево регрессии          | Улучшенный бейзлайн        | 0.7668 | 0.0524 |
| Дерево регрессии          | Имплементация              | 0.5800 | 0.0700 |
| Дерево регрессии          | Улучшенная имплементация   | 0.7600 | 0.0500 |
| Случайный лес             | Бейзлайн                   | 0.8066 | 0.0498 |
| Случайный лес             | Улучшенный бейзлайн        | 0.8179 | 0.0473 |
| Случайный лес             | Имплементация              | 0.7877 | 0.0496 |
| Случайный лес             | Улучшенная имплементация   | 0.8007 | 0.0483 |
| Бустинг                   | Бейзлайн                   | 0.7955 | 0.0502 |
| Бустинг                   | Улучшенный бейзлайн        | 0.8125 | 0.0478 |
| Бустинг                   | Имплементация              | 0.7949 | 0.0502 |
| Бустинг                   | Улучшенная имплементация   | 0.8107 | 0.0478 |



В течение курса мною были выполнены пять лабораторных работ, целью которых было применить на практических задачах регрессии и классификацииразличные методы машинного обученя.  

Первая работа была направлена на изучение метода KNN.  
Суть алгоритма **KNN** заключается не в построении какой-либо глобальной модели, как в случае с линейными моделями или деревьями, а в использовании локального подхода: для предсказания значения или класса новой точки данных алгоритм просто ищет k ближайших к ней точек из обучающего набора. В задаче классификации k-NN присваивает новой точке класс, который является наиболее частым среди ее k ближайших соседей. В задаче регрессии алгоритм предсказывает значение как среднее арифметическое значений целевой переменной у этих k соседей. На практике KNN может давать хорошие результаты, особенно когда границы классов или регрессионная зависимость имеют сложную, но локально-структурированную форму.  

Во второй и третьей работе были проведены эксперементы с **линейными алгоритмами** - линейной и логистической регрессией.  
Суть алгоритма **линейной регрессии** в построении регрессионной прямой через облако точек данных так, чтобы минимизировать функцию ошибки, которая пропорциональна сумме расстояний от точек до прямой. Этот алгоритм не обладает большой выразительной способностью из-за своей линейной природы. Аналогично **логистическая регрессия** строит линейную поверхность, которая делит облако точек на две части, тем самым относительно которой создается порог классификации на основе удаленности точек от нее. Эти алгоритмы показывали хорошую точность классификации и регрессии, но чтобы улучшить эти показатели, необходимо пользоваться нелинейными алгоритмами.  

В следующей работе я использовал дерево решений для решения задач классификации.  
**Дерево решений** строит нелинейную кусочно-заданную поверхность на основе критерия разделения. На каждом шаге пространство данных делится на части по определенному критерию. В случае регрессии сплит происходит так, чтобы минимизировать расстояния от точек до поверхности предсказания, а в задачах регрессии сплит делит данные так, чтобы уменьшить меру хаоса в них - энтропию. Дерево решений очень склонно к переобучению, если строить его неограниченно. Переобученное дерево очень глубокое и имеет много листов, причем переобученность может дойти до того, что в каждом листе будет по одному экземпляру обучающих данных.  Дерево решений способно строить нелинейные поверхности, поэтому для нас это свойство позволило улучшить метрики.  

Следущим этапом было использование ансамблевых алгоритмов, а именно случайного леса и градиентного бустинга.  
Ансамблевые алгоритмы используются для улучшения выразительной способности отдельных деревьев - эстиматоров. **Случайный лес** строит несколько деревьев на подвыборках данных и подвыборках признаков, а потом усредняет их показания. Смысл случайного леса - построить шумные и глубокие, т.е переобученные деревья, а потом усреднить этот шум, ведь теоретически наши данные имеют нормальный шум относительно теоретического решения, а значит усредняя решения, построенные на разных подвыборках, мы приблизимся к эталоному решению. Следовательно случайный лес отлично решает проблему переобучения дерева решений. У градиентного бустинга же другая идея. **Градиентный бустинг** - это линейная комбинация нескольких решений, где каждое последующее строится на ошибках линейной комбинации предыдущих. Таким образом каждое последующее дерево исправляет ошибки предыдущих. Это улучшает выразительную способность модели, но не устраняет свойство переобучаться. В нашей работе самые лучшие результаты я получил используя бустинг и случайный лес.